<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
     More info: h5bp.com/i/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <meta name="google-site-verification" content="s9HfQLacjwqz4eRF6EoYjc99aE5T00xyFcCA3zIniwc" />

  <!-- Our site title and description -->
  <link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
  <title>Xiaohao Sun | 孙小皓</title>

  <meta name="generator" content="DocPad v6.79.4" />

  <!-- Mobile viewport optimized: h5bp.com/viewport -->
  <meta name="viewport" content="width=device-width" />

  <!-- Shims: IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script async src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/styles/twitter-bootstrap.css" />
  <link rel="stylesheet" href="/styles/style.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
  <script src="/scripts/bootstrap.min.js"></script>
  <script src="/scripts/script.js"></script>
</head>

<body>
  <div class="container">
    <section id="content" class="content">
      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"></a>
          </div>

          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav">
              <li><a href="index.html">Home</a></li>
              <li><a href="pubs.html">Publications</a></li>
              <li><a href="about.html">About</a></li>
              <li><a href="contact.html">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->
        </div><!-- /.container-fluid -->
      </nav>

      <div class="row">
        <div class="col-sm-12">
          <div class="row">
            <div class="col-sm-8">
              <!-- <div style="float: right">
    <audio controls autoplay loop height="50" width="50">
        <source src="music/yiliaobailiao.mp3" type="audio/mpeg">
        <embed height="50" width="100" src="music/yiliaobailiao.mp3">
    </audio>
</div> -->
              <!-- <img src=files/name.png alt="image" class="img-responsive" style="width: 700px;" /> -->
              <h1>Xiaohao Sun | 孙小皓</h1>
              <h5>Stay humble, trust your instincts. Most importantly, act. When you come to a fork in the road, take it.</h5>
            </div>
            <div class="col-sm-4 text-right">
              <img src="files/SFU.png" width="100" alt="" style="border-style: none;" align="top">
            </div>
          </div>
          <hr>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-8 pull-left" style="min-height: 280px; display: flex; flex-direction: column;">
          <p>
            I'm currently a Ph.D. student of computer science at <a href="https://www.sfu.ca">Simon Fraser
              University</a>, advised by professor
            <a href="https://angelxuanchang.github.io">Angel Xuan Chang</a>. Prior to this, I got my Master of Applied
            Science degree from the Electrical Engineering Department
            <a href="https://www.uwindsor.ca/">University of Windsor</a>. And I received my Bachelor of Science degree
            in the area of Mathematical and Physics Basic Science at School of Mathematical from <a
              href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China</a>.
            Currently I am also a research assistant in <a href="https://gruvi.cs.sfu.ca/people/">SFU GrUVi Lab</a>
            working with professor <a href="https://angelxuanchang.github.io">Angel
              Xuan Chang</a> at <a href="https://www.sfu.ca">Simon Fraser University</a>.
            My research interests span 3D vision and language, generative models, and artificial intelligence, with a specific focus on 3D scene understanding and synthesis.
          </p>
          <p>
          <h3>News</h3>
          <ul>
            <li> July, 2025 - Passed my PhD Thesis Proposal</li>
            <li> June, 2025 - Passed my PhD depth exam</li>
            <li> Oct, 2023 - One paper got accpeted at <a href="https://3dvconf.github.io/2024/">3DV 2024</a> as Oral
            </li>
            <li> Aug, 2022 - One paper got accpeted at <a href="https://3dvconf.github.io/2022/">3DV 2022</a></li>
            <li> Jan, 2022 - I joined GrUVi Lab, and start to work with Prof <a
                href="https://angelxuanchang.github.io">Angel Xuan Chang</a></li>
            <li> Sep, 2021 - Start my CS PHD at <a href="https://www.sfu.ca/">SFU</a></li>
          </ul>
          <a href="news.html">More...</a>
          </p>
        </div>
        <div class="col-sm-4 text-right pull-right">
          <img src="files/xiaohao_sun.jpg" alt="Xiaohao Sun" width="220px" style="border: 3px solid #ddd; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" />
          <div style="font-family:monospace;">
            xiaohao_sun-{at}-sfu-"dot"-ca
          </div><br />

          PhD Student<br />
          Sep 2021 - Present<br />
          <a href="https://www.sfu.ca">Simon Fraser University</a><br />
          Research Assistant (<a href="https://gruvi.cs.sfu.ca/people/">Gruiv</a> & <a
            href="https://angelxuanchang.github.io/group.html">3dlg</a>)<br />
          <a href="https://scholar.google.com/citations?user=pQhGyqMAAAAJ&hl=en">Google Scholar</a><br />
          <a href="https://twitter.com/XiaohaoSun4">Twitter</a><br />
        </div>
      </div>
      <br>

      <div class="row">
        <div class="col-sm-12">
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2>Publications</h2>
            </div>
            <div class="panel-body">


              <div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
                <div class="col-sm-3 paper-img">
                  <a href="#"><img src=files/semlayoutdiff.png alt="image" class="img-responsive"
                      style="width: 300px;" /></a>
                  &nbsp;
                </div>
                <div class="col-sm-9">
                  <h4 class="red">SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis</h4>
                  <strong>Xiaohao Sun</strong>,
                  <a href=https://dv-fenix.github.io/ target="_blank">Divyam Goel</a>,
                  <a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a><br>
                  <a href=https://arxiv.org/pdf/2508.18597 target= "_blank">arXiv</a>
                  <p>We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor scenes across multiple room types. 
                    The model introduces a scene layout representation combining a top-down semantic map and attributes for each object. 
                    Unlike prior approaches, which cannot condition on architectural constraints, SemLayoutDiff employs a categorical diffusion model capable of conditioning scene synthesis explicitly on room masks. 
                    It first generates a coherent semantic map, followed by a cross-attention-based network to predict furniture placements that respect the synthesized layout. 
                    Our method also accounts for architectural elements such as doors and windows, ensuring that generated furniture arrangements remain practical and unobstructed. 
                    Experiments on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent, realistic, and varied scenes, outperforming previous methods.</p>
                  <a href=https://arxiv.org/pdf/2508.18597 target="_blank">[Paper]</a>
                  <a href=https://3dlg-hcvc.github.io/SemLayoutDiff/ target="_blank">[Project]</a>
                  <a href=https://github.com/3dlg-hcvc/SemLayoutDiff target="_blank">[Code]</a>
                </div>
              </div>
              <div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
                <div class="col-sm-3 paper-img">
                  <a href="#"><img src=files/opdmulti.png alt="image" class="img-responsive"
                      style="width: 300px;" /></a>
                  &nbsp;
                </div>
                <div class="col-sm-9">
                  <h4 class="red">OPDMulti: Openable Part Detection for Multiple Objects</h4>
                  <strong>Xiaohao Sun*</strong>,
                  <a href=https://jianghanxiao.github.io/ target="_blank">Hanxiao Jiang*</a>,
                  <a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
                  <a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
                  <a href=https://3dvconf.github.io/2024/ target="_blank">3DV 2024, <b><i
                        style="color: #ff0000;">Oral</i></b></a>
                  <p>Openable part detection is the task of detecting the openable parts of an object in a single-view
                    image, and predicting corresponding motion parameters. Prior work investigated the unrealistic
                    setting where all input images only contain a single openable object. We generalize this task to
                    scenes with multiple objects each potentially possessing openable parts, and create a corresponding
                    dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a
                    part-aware transformer architecture. Our experiments show that the OPDFormer architecture
                    significantly outperforms prior work. The more realistic multiple-object scenarios we investigated
                    remain challenging for all methods, indicating opportunities for future work.</p>
                  <a href=https://arxiv.org/abs/2303.14087 target="_blank">[Paper]</a>
                  <a href=https://3dlg-hcvc.github.io/OPDMulti/ target="_blank">[Project]</a>
                  <a href=https://github.com/3dlg-hcvc/OPDMulti target="_blank">[Code]</a>
                </div>
              </div>
              <div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
                <div class="col-sm-3 paper-img">
                  <a href="#"><img src=files/articulated-3DHOI.png alt="image" class="img-responsive"
                      style="width: 300px;" /></a>
                  &nbsp;
                </div>
                <div class="col-sm-9">
                  <h4 class="red">Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of
                    Approaches and Challenges</h4>
                  <a href=https://www.sanjayharesh.com/ target="_blank">Sanjay Haresh</a>,
                  <strong>Xiaohao Sun</strong>,
                  <a href=https://jianghanxiao.github.io/ target="_blank">Hanxiao Jiang</a>,
                  <a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
                  <a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
                  <a href=https://3dvconf.github.io/2022/ target="_blank">3DV 2022</a>
                  <p>Human-object interactions with articulated objects are common in everyday life. Despite much
                    progress in single-view 3D reconstruction, it is still challenging to infer an articulated 3D object
                    model from an RGB video showing a person manipulating the object. We canonicalize the task of
                    articulated 3D human-object interaction reconstruction from RGB video, and carry out a systematic
                    benchmark of four methods for this task: 3D plane estimation, 3D cuboid estimation, CAD model
                    fitting, and free-form mesh fitting. Our experiments show that all methods struggle to obtain high
                    accuracy results even when provided ground truth information about the observed objects. We identify
                    key factors which make the task challenging and suggest directions for future work on this
                    challenging 3D computer vision task.</p>
                  <a href=https://arxiv.org/abs/2209.05612 target="_blank">[Paper]</a>
                  <a href=https://3dlg-hcvc.github.io/3dhoi/ target="_blank">[Project]</a>
                  <a href=https://github.com/3dlg-hcvc/3dhoi target="_blank">[Code]</a>
                </div>
              </div>
              <div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
                <div class="col-sm-3 paper-img">
                  <a href="#"><img src=files/read_line.png alt="image" class="img-responsive"
                      style="width: 300px;" /></a>
                  &nbsp;
                </div>
                <div class="col-sm-9">
                  <h4 class="red">Reading Line Classification Using Eye-trackers</h4>
                  <strong>Xiaohao Sun</strong>,
                  <a href=https://www.singamlabs.org/ target="_blank">Balakumar Balasingam</a><br>
                  <a href=https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9259274 target="_blank">IEEE TIM</a>
                  <p>Eye-tracking while reading is an emerging application where the goal is to track the progression of
                    reading. The challenges for accurate tracking of the reading progression are due to the measurement
                    noise of the eye-tracker and the rapid and uncertain movement of the eye gaze. Solutions to this
                    problem developed in the recent past suffer from many limitations, such as the need to know the text
                    context and the need to have a batch of one page of data for classification. In this article, we
                    relax these assumptions and develop a novel, real-time line classification approach. The proposed
                    solution consists of an improved slip-Kalman smoother (slip-KS) that is designed to detect new line
                    returns and to reduce the variance in the eye-gaze measurements. After preprocessing of the data by
                    the slip-KS, a classification approach is employed to track the lines being read in real-time. Two
                    such classifiers are demonstrated in this article; one is based on Gaussian discriminants, and the
                    other is based on support vector machines. The proposed approaches were tested using realistic
                    eye-gaze data from seven participants. Analysis based on the collected data using the proposed
                    algorithms shows significantly improved performance over existing methods.</p>
                  <a href=https://ieeexplore.ieee.org/document/9475049 target="_blank">[Paper]</a>
                </div>
              </div>
              <div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
                <div class="col-sm-3 paper-img">
                  <a href="#"><img src=files/KS.png alt="image" class="img-responsive" style="width: 300px;" /></a>
                  &nbsp;
                </div>
                <div class="col-sm-9">
                  <h4 class="red">Algorithms for Reading Line Classification</h4>
                  <strong>Xiaohao Sun</strong>,
                  <a href=https://www.singamlabs.org/ target="_blank">Balakumar Balasingam</a><br>
                  <a href=http://ieeesmc2021.org/ target="_blank">IEEE SMC</a>
                  <p>Eye-Tracking has been emerging as a useful tool in human-computer interaction. However, the state
                    of the art in eye-tracking applications suffers from a significant amount of measurement noise.
                    Also, the inherent nature of the eye-gaze movement adds to the difficulty of obtaining valuable
                    information from eye-gaze measurements. In this paper, a novel classification approach is proposed
                    to classify the lines being read based on eye-gaze measurements. The proposed approach consists of a
                    novel Kalman smoother-based preprocessing procedure to separate eye-gaze data corresponding to
                    different text lines and to reduce variance. The preprocessed data is then used to train two
                    different classifiers, one based on Gaussian discriminants and the other based on support vector
                    machines. The resulting line-classification approach is shown to be superior in performance compared
                    to other recent approaches.</p>
                  <a href=https://ieeexplore.ieee.org/abstract/document/9658688 target="_blank">[Paper]</a>
                </div>
              </div>

            </div>
          </div>
        </div>

      </div>
  </div>
  </section>
  <!-- <footer>
    <p class="pull-right">
      Last updated at 2020-10-07T10:56:51.321Z
    </p>
  </footer> -->
  </div><!-- /container -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RX00NTCBPM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-RX00NTCBPM');
  </script>

</body>

</html>